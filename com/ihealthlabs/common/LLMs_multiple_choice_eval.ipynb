{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating LLMsâ€˜ Performance in Answering RD and CDCES Exam Prep Questions\n",
    "This notebook provides code examples to get the questions from the database and ask the questions in batch to the LLMs. \n",
    "The LLMs evaluated in this project are \n",
    "- GPT 4/4o\n",
    "- Gemini 1.5 Pro\n",
    "- Claude 3 - Opus\n",
    "\n",
    "We also tried Claude 3 - Haiku, Claude 3 - Sonnet, and Llama 3 8B, but due to the low accuracy rate, we didn't continue the evaluation and analysis of these models. The results will also be included in the notebook.\n",
    "\n",
    "## Setup\n",
    "Before running the rest of this notebook, you'll need to run the cells below to ensure necessary libaraies are installed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install openai\n",
    "#%pip install boto3\n",
    "#%pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to store your enviromental variables in the .env file\n",
    "Here is the sample .env file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYSQL_USER=<Your_MySQL_User>\n",
    "MYSQL_PASSWORD=<Your_MySQL_Password>\n",
    "MYSQL_HOST=<Your_MySQL_Host>\n",
    "MYSQL_PORT=<Your_MySQL_Port>\n",
    "DB_NAME=<Your_Database_Name>\n",
    "OPENAI_API_KEY=<Your_OPENAI_API_Key>\n",
    "GEMINI_API_KEY=<Your_GEMINI_API_Key>\n",
    "GOOGLE_CLOUD_PROJECT=<Your_Google_Cloud_Project_ID>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to set AWS access. Please follow the instruction here: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to OPENAI API for GPT 4o to Answer RD/CDCES Exam Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai_api\n",
    "import questions_mysql\n",
    "\n",
    "api = openai_api.OpenAIAPI()\n",
    "qsql = questions_mysql.QuestionsMysql()\n",
    "# Connect to RD Exam Questions\n",
    "question_dict = qsql.get_RD_questions()\n",
    "# Connect to CDCES Exam Questions\n",
    "# question_dict = qsql.get_questions()\n",
    "\n",
    "response4 = \"\\n\"\n",
    "\n",
    "# Prompt the questions in a batch of 1, you can adjust the question number in each batch \n",
    "for startIndex in range (1, len(question_dict) + 1, 1):\n",
    "    prompt_str = qsql.get_prompt_string(question_dict, startIndex, 1)\n",
    "    print(prompt_str)\n",
    "    # Use the GPT 4o model and set temperature to 0. To use GPT 3.5 ot GPT 4 model, change the \"gpt-4o\" to \"gpt-4 or gpt-3.5\"\n",
    "    response4 += api.ask_chatgpt(prompt_str, \"gpt-4o\", 0) \n",
    "    response4 += \"\\n\"\n",
    "\n",
    "score4 = qsql.get_score(response4, question_dict)\n",
    "print(\"\\n\")\n",
    "print('gpt4o:' + score4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Amazon BedRock API for Claude 3 - Opus to Answer RD/CDCES Exam Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic_bedrock_api\n",
    "import questions_mysql\n",
    "\n",
    "api = anthropic_bedrock_api.AnthropicBedRockAPI()\n",
    "qsql = questions_mysql.QuestionsMysql()\n",
    "# Connect to RD Exam Questions\n",
    "question_dict = qsql.get_RD_questions()\n",
    "# Connect to CDCES Exam Questions\n",
    "# question_dict = qsql.get_questions()\n",
    "\n",
    "response_claude = \"\"\n",
    "\n",
    "# Prompt the questions in a batch of 1, you can adjust the question number in each batch \n",
    "for startIndex in range (1, len(question_dict) + 1, 1):\n",
    "    prompt_str = qsql.get_prompt_string(question_dict, startIndex, 1)\n",
    "    print(prompt_str)\n",
    "    # Use the Claude 3 - Opus model and set temperature to 0. \n",
    "    # To use Claude 3 Haiku/Sonnet, use \"anthropic.claude-3-haiku-20240307-v1:0\" or \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    response_claude += api.ask_claude(prompt_str, \"anthropic.claude-3-opus-20240229-v1:0\", 0)\n",
    "    response_claude += \"\\n\"\n",
    "\n",
    "score_claude = qsql.get_score(response_claude, question_dict)\n",
    "print(\"\\n\")\n",
    "print('Claude 3 Opus: ' + score_claude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Gemini API for Gemini 1.5 Pro to Answer RD/CDCES Exam Questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gemini_api\n",
    "import questions_mysql\n",
    "\n",
    "api = gemini_api.GeminiAIAPI()\n",
    "qsql = questions_mysql.QuestionsMysql()\n",
    "# Connect to RD Exam Questions\n",
    "question_dict = qsql.get_RD_questions()\n",
    "# Connect to CDCES Exam Questions\n",
    "# question_dict = qsql.get_questions()\n",
    "\n",
    "prompt_str = \"\"\n",
    "\n",
    "response = \"\\n\"\n",
    "# Prompt the questions in a batch of 1, you can adjust the question number in each batch \n",
    "for startIndex in range (1, len(question_dict) + 1, 1):\n",
    "    prompt_str = qsql.get_prompt_string(question_dict, startIndex,  1)\n",
    "    print(prompt_str)\n",
    "    # Use the Gemini 1.5 Pro model and set temperature to 0. \n",
    "    response += api.ask_gemini(prompt_str, 'gemini-1.5-pro', 0)\n",
    "\n",
    "\n",
    "score = qsql.get_score(response, question_dict)\n",
    "print(\"\\n\")\n",
    "print('Gemini 1.5 Pro :' + score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Amazon BedRock API for Llama 3 8B to Answer RD/CDCES Exam Questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_bedrock_api\n",
    "import questions_mysql\n",
    "\n",
    "api = llama_bedrock_api.LlamaBedRockAPI()\n",
    "qsql = questions_mysql.QuestionsMysql()\n",
    "# Connect to RD Exam Questions\n",
    "question_dict = qsql.get_RD_questions()\n",
    "# Connect to CDCES Exam Questions\n",
    "# question_dict = qsql.get_questions()\n",
    "\n",
    "response_llama = \"\"\n",
    "\n",
    "# Prompt the questions in a batch of 1, you can adjust the question number in each batch \n",
    "for startIndex in range (1, len(question_dict) + 1, 1):\n",
    "    prompt_str = qsql.get_prompt_string_llama(question_dict, startIndex, 1)\n",
    "    # Use the llama 3 8B model and set temperature to 0.\n",
    "    response_llama += api.ask_llama(prompt_str, \"meta.llama3-8b-instruct-v1:0\", 0)\n",
    "    response_llama += \"\\n\"\n",
    "\n",
    "score_llama = qsql.get_score(response_llama, question_dict)\n",
    "print(\"\\n\")\n",
    "print('Llama 3 8B: ' + score_llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results & Analysis\n",
    "The results of RD/CDCES Exam Questions of each model can be found: https://www.notion.so/Response-from-AIs-for-Multiple-Choice-Questions-f0b87ff56e0e4ae99101db305f8f3596\n",
    "\n",
    "Detailed Question Investigation & Analysis for RD Exam Questions can be found: https://docs.google.com/spreadsheets/d/1ApqiEV2WM4n7Ytz5rNTgoUvWb5PT2MQlzMovsK3lZ38/edit#gid=1209999158\n",
    "\n",
    "Analysis Summay Slides can be found: https://docs.google.com/presentation/d/1gtmfSIIMp7jbRxFJ3DS_iQVBMiVdeINyT6PcpF7Dj98/edit#slide=id.g273d235a019_0_139 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
